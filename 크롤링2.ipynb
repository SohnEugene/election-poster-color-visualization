{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t93N0p8c3nlO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 페키지 임포트, 함수 정의 등"
      ],
      "metadata": {
        "id": "t93N0p8c3nlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3XE5za8Xkrz",
        "outputId": "8c0b8028-2554-43fc-e3ff-8a8a7adb5760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 정의\n",
        "\n",
        "# folder_path에 url에서 받아온 pdf를 filename으로 저장하는 함수\n",
        "def download_pdf(url, folder_path, filename, party):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        #filename = url.split(\"/\")[-1]\n",
        "        file_path = os.path.join(folder_path+party, filename)\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {url}\")\n",
        "\n",
        "\n",
        "# 실제 크롤링을 수행하는 함수\n",
        "def scrape_candidates(url, folder_path, parties, order):\n",
        "\n",
        "    order = order\n",
        "    page = 1\n",
        "\n",
        "    while (True) :\n",
        "        if (page%10 == 0) :\n",
        "            print(order, page)\n",
        "\n",
        "        response = requests.get(url.format(page, order))\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser').select_one('#content > div.searchList > ul')\n",
        "\n",
        "            if soup == None :\n",
        "                return\n",
        "\n",
        "            for li in soup.find_all('li', class_='clear'):\n",
        "                name = li.find('strong').text.strip()\n",
        "                info = li.find('p', class_='l10 t10').text.strip()\n",
        "                party = info.split(\"|\")[-2].strip()\n",
        "\n",
        "                if party in parties:\n",
        "                    pdf_links = li.find_all('a', href=lambda href: href)\n",
        "                    for link in pdf_links:\n",
        "                        pdf_url = urljoin(url, link['href'])\n",
        "                        # 모든 다운로드 링크 확인\n",
        "                        if \"벽보\" in link.img['alt'] and \"viewPdf\" not in pdf_url:\n",
        "                            filename = info + \" | \" + name\n",
        "                            download_pdf(pdf_url, folder_path, filename, party)\n",
        "                            break\n",
        "\n",
        "        page += 1"
      ],
      "metadata": {
        "id": "o4Zug-B_Xn9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스크래핑할 선관위 도서관 링크\n",
        "\n",
        "총선 = \"https://library.nec.go.kr/neweps/3/1/paperSearch.do?chs=&page={0}&epType=&start_file_sn=&end_file_sn=&svc_path_nm=&epid=&candidate_nm=&elect_ymd=&epdata_id=&ctl_no_id=&elect_sn=&party_nm=&win_yn=&elecDe=&turnDe=%B1%B9%C8%B8%C0%C7%BF%F8%BC%B1%B0%C5&elecType=20&code_id=20++&elecTypeMax=30&turnType=EPS0420++&cityType=EPS0420{1}&guType=&sign_id={1}&sido_nm=&elect_region_nm=&fieldName=candidate_nm&category=tms120tbl&myScraps=&kwd=&elecTypes=20&electionType=on&electionTurn=on&pageSize=10&order=elect_ymd&order_sort=desc&m_score=%B8%C5%BF%EC%B8%B8%C1%B7\"\n",
        "광역 = \"https://library.nec.go.kr/neweps/3/1/paperSearch.do?chs=&page={0}&epType=&start_file_sn=&end_file_sn=&svc_path_nm=&epid=&candidate_nm=&elect_ymd=&epdata_id=&ctl_no_id=&elect_sn=&party_nm=&win_yn=&elecDe=&turnDe=%B1%A4%BF%AA%B4%DC%C3%BC%C0%E5%BC%B1%B0%C5&elecType=30&code_id=30++&elecTypeMax=50&turnType=EPS0430++&cityType=EPS04300{1}&guType=&sign_id={1}&sido_nm=&elect_region_nm=&fieldName=candidate_nm&category=tms120tbl&myScraps=&kwd=&elecTypes=30&electionType=on&electionTurn=on&pageSize=10&order=elect_ymd&order_sort=desc&m_score=%B8%C5%BF%EC%B8%B8%C1%B7\"\n",
        "기초 = \"https://library.nec.go.kr/neweps/3/1/paperSearch.do?chs=&page={0}&epType=&start_file_sn=&end_file_sn=&svc_path_nm=&epid=&candidate_nm=&elect_ymd=&epdata_id=&ctl_no_id=&elect_sn=&party_nm=&win_yn=&elecDe=&turnDe=%B1%E2%C3%CA%B4%DC%C3%BC%C0%E5%BC%B1%B0%C5&elecType=30&code_id=40++&elecTypeMax=50&turnType=EPS0440++&cityType=EPS04400{1}&guType=&sign_id={1}&sido_nm=&elect_region_nm=&fieldName=candidate_nm&category=tms120tbl&myScraps=&kwd=&elecTypes=30&electionType=on&electionTurn=on&pageSize=10&order=elect_ymd&order_sort=desc&m_score=%B8%C5%BF%EC%B8%B8%C1%B7\""
      ],
      "metadata": {
        "id": "vJnlzY-AYegh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 스크래핑 실행 코드"
      ],
      "metadata": {
        "id": "Gg6Mbxes3t6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 총선 벽보 pdf 파일 스크래핑\n",
        "\n",
        "url = 총선\n",
        "orders = [21, 20, 19, 18, 17, 16]\n",
        "party_list = [[\"더불어민주당\", \"국민의힘\"],\n",
        "              [\"더불어민주당\", \"새누리당\", \"국민의당\"],\n",
        "              [\"새누리당\", \"민주통합당\"],\n",
        "              [\"한나라당\", \"통합민주당\"],\n",
        "              [\"열린우리당\", \"한나라당\"],\n",
        "              [\"한나라당\", \"새천년민주당\"]] # 교섭단체(20석 이상) 정당만\n",
        "\n",
        "folder_path = \"\" # pdf파일을 저장할 폴더 경로\n",
        "\n",
        "for i in range(len(orders)) :\n",
        "    new_path = folder_path + str(orders[i]) + \"대 총선 \"\n",
        "    scrape_candidates(url, new_path, party_list[i], orders[i])"
      ],
      "metadata": {
        "id": "B-IRPzFj2b8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 지선 광역 벽보 pdf 파일 스크래핑\n",
        "\n",
        "url = 광역\n",
        "orders = [8, 7, 6, 5, 4, 3]\n",
        "party_list = [[\"더불어민주당\", \"국민의힘\"],\n",
        "              [\"더불어민주당\", \"자유한국당\"],\n",
        "              [\"새정치민주연합\", \"새누리당\"],\n",
        "              [\"민주당\", \"한나라당\"],\n",
        "              [\"민주당\", \"열린우리당\", \"한나라당\"],\n",
        "              [\"새천년민주당\", \"한나라당\"]]\n",
        "\n",
        "folder_path = \"\"\n",
        "\n",
        "for i in range(len(orders)) :\n",
        "    new_path = folder_path + str(orders[i]) + \"대 지선 광역 \"\n",
        "    scrape_candidates(url, new_path, party_list[i], orders[i])"
      ],
      "metadata": {
        "id": "k6i9pCJ436BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 지선 기초 벽보 pdf 파일 스크래핑\n",
        "\n",
        "url = 기초\n",
        "orders = [8, 7, 6, 5, 4, 3]\n",
        "party_list = [[\"더불어민주당\", \"국민의힘\"],\n",
        "              [\"더불어민주당\", \"자유한국당\"],\n",
        "              [\"새정치민주연합\", \"새누리당\"],\n",
        "              [\"민주당\", \"한나라당\"],\n",
        "              [\"민주당\", \"열린우리당\", \"한나라당\"],\n",
        "              [\"새천년민주당\", \"한나라당\"]]\n",
        "\n",
        "folder_path = \"\"\n",
        "\n",
        "for i in range(len(orders)) :\n",
        "    new_path = folder_path + str(orders[i]) + \"대 지선 기초 \"\n",
        "    scrape_candidates(url, new_path, party_list[i], orders[i])"
      ],
      "metadata": {
        "id": "xLeddfMc4iRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}